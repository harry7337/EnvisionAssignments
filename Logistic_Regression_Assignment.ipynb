{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harry7337/EnvisionAssignments/blob/main/Logistic_Regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hand-Written Digit Classification\n",
        "\n",
        "Your task is to classify handwritten digits from the mnist dataset.\n",
        "\n",
        "The aim is to use logistic regression for this task. This would mean using fully connected aka only Dense layers.\n",
        "\n",
        "Follow the comments below to complete the assignment and feel free to play around with the hyper-parameters to obtain a good accuracy!"
      ],
      "metadata": {
        "id": "KY5JpZ4utck6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense # here you can import other layers as well\n",
        "from keras.models import Sequential #feel free to google how to use these functions\n",
        "# import relevant libraries"
      ],
      "metadata": {
        "id": "Js5kBKuEtYf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lincuAlwY6Ts"
      },
      "source": [
        "# choose mnist dataset\n",
        "mnist=keras.datasets.mnist\n",
        "\n",
        "# loading data set \n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3i1jZxbZJqg"
      },
      "source": [
        "# analyse the shape of the training and testing dataset\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you might want to reshape xtrain as we will be using only dense layers in our model"
      ],
      "metadata": {
        "id": "udOJNubOnXsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Ps-AuTZxms"
      },
      "source": [
        "model = Sequential([\n",
        "                    #  add layers here\n",
        "                    # we would recommend to start with 4 dense layers. Start with number of neurons in hidden layers = 16,32,64\n",
        "                    # no. of neurons in output layer is equal to the total no. of classes, which is?\n",
        "                      ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "542BCviObz3e"
      },
      "source": [
        "# use this to view the summary of your model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm6na6MRcrnk"
      },
      "source": [
        "# use the compile method with adam optimizer(default learning rate for adam=0.001)\n",
        "# the loss function will be crossentropy but will we use sparse or categorical\n",
        "# use accuracy as the metric\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smVkIgOReUzR"
      },
      "source": [
        "# Pass in x_train,y_train for 10 epochs and use as validation_data=(x_test,y_test)\n",
        "# we use history object so that we can use it later to analyse loss\n",
        "history=model.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMb8fuoIfOcq"
      },
      "source": [
        "imgIndex=500 # change this number to anything(within the test dataset limit ofc)\n",
        "img = x_test[imgIndex]\n",
        "plt.imshow(img.reshape(1,28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-THDIY2fi7z"
      },
      "source": [
        "pred = model.predict(img.reshape(1,28*28))[0]\n",
        "np.argmax(pred) # this gives us the index of the highest probability, try printing the pred array to look at the output of the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CEJPEWXiYWv"
      },
      "source": [
        "# this is the code snippet for analysing the variation of loss and accuracy with respect to epochs\n",
        "plt.figure()\n",
        "fig,(a1,a2)=plt.subplots(1,2,figsize=(19,8))\n",
        "\n",
        "a1.plot(history.history['loss'])\n",
        "a1.plot(history.history['val_loss'])\n",
        "a1.legend(['Training loss', 'Validation loss'])\n",
        "\n",
        "a2.plot(history.history['accuracy'])\n",
        "a2.plot(history.history['val_accuracy'])\n",
        "a2.legend(['Training accuracy', 'Validation accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxIE3Y7jT3-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
